# Thinking Token Routing Fix ‚úÖ

## Problem
The backend was sending the **final formatted answer** as `thinking_token` events instead of `answer_token` events. This caused the full markdown response (with headers like `### Most Common Hazard Analysis`) to appear in the "Thinking" dropdown instead of the main answer display.

### What Was Happening
```
Backend sends:
  thinking_token: "### Most Common Hazard Analysis\n\nBased on..."
  
Frontend displayed:
  ‚úÖ Thinking dropdown: "### Most Common Hazard Analysis..." ‚ùå WRONG!
  ‚ùå Main answer: (empty)
```

## Solution
Implemented **automatic mode detection** that switches from "thinking mode" to "answer mode" when markdown formatting is detected in the token stream.

### How It Works

#### 1. **Mode Tracking with useRef**
```tsx
const isAnswerModeRef = useRef<boolean>(false);
```
- Tracks whether we've switched to answer mode
- Uses ref for immediate updates (no async delays)
- Reset to `false` when starting new message

#### 2. **Markdown Detection**
```tsx
if (data.type === 'thinking_token' && data.token) {
  // Detect markdown formatting (indicates final answer)
  if (!isAnswerModeRef.current && 
      (data.token.includes('###') || 
       data.token.includes('##') || 
       data.token.includes('**'))) {
    isAnswerModeRef.current = true;
    console.log('üîÑ Switched to answer mode');
  }
  
  if (isAnswerModeRef.current) {
    // Route to final answer display
    setCurrentAnalysis(prev => (prev || "") + data.token!);
    setFinalAnswer(prev => (prev || "") + data.token!);
  } else {
    // Keep in thinking box for actual reasoning
    setThinkingText(prev => prev + data.token!);
  }
}
```

#### 3. **Detection Triggers**
The system switches to answer mode when it detects:
- `###` - Markdown H3 headers
- `##` - Markdown H2 headers  
- `**` - Bold text (common in formatted answers)

Once switched, **all subsequent tokens** go to the final answer display.

## Behavior

### Before Fix ‚ùå
```
Thinking dropdown:
  ### Most Common Hazard Analysis
  Based on the safety data...
  [Full formatted response]

Main answer area:
  (empty)
```

### After Fix ‚úÖ
```
Thinking Process dropdown (collapsed):
  (Only shows if there was actual reasoning before markdown)

Main answer area:
  ### Most Common Hazard Analysis
  Based on the safety data...
  [Full formatted response with proper markdown rendering]
```

## UI Updates

### Thinking Dropdown
- **Label**: Changed to "Thinking Process" with "Internal Reasoning" badge
- **Default state**: Collapsed (not auto-expanded)
- **Visibility**: Only shows if there's actual thinking content (before markdown switch)
- **Purpose**: Shows internal reasoning/planning (if backend sends it separately)

### Main Answer Display
- **Receives**: All tokens after markdown detection
- **Rendering**: Full markdown support with ReactMarkdown
- **Streaming**: Character-by-character with animated cursor
- **Formatting**: Proper headings, bold, lists, etc.

## Edge Cases Handled

### 1. **No Thinking Phase**
If backend immediately sends markdown:
- `isAnswerModeRef` switches on first token
- Thinking dropdown never appears
- All content goes to main answer ‚úÖ

### 2. **Actual Thinking + Answer**
If backend sends reasoning then answer:
```
thinking_token: "Let me analyze the data..."
thinking_token: "I'll look for patterns..."
thinking_token: "### Most Common Hazard" ‚Üê SWITCH HERE
thinking_token: "Analysis\n\nBased on..."
```
- First tokens ‚Üí Thinking dropdown
- After markdown ‚Üí Main answer
- Both sections visible ‚úÖ

### 3. **Multiple Messages**
- `isAnswerModeRef.current = false` on each new message
- Fresh detection for each response
- No cross-contamination ‚úÖ

## Console Logs

```
üì° WebSocket message: { type: 'thinking_token', token: 'Let me' }
üì° WebSocket message: { type: 'thinking_token', token: ' analyze' }
üì° WebSocket message: { type: 'thinking_token', token: '###' }
üîÑ Switched to answer mode - routing to final answer display
üì° WebSocket message: { type: 'thinking_token', token: ' Most' }
üì° WebSocket message: { type: 'thinking_token', token: ' Common' }
```

## Backend Compatibility

### Works With
‚úÖ `thinking_token` only (current backend)  
‚úÖ `answer_token` only (proper implementation)  
‚úÖ Mixed `thinking_token` + `answer_token`  
‚úÖ `final_answer` / `final` events  

### Detection Patterns
- `###` - H3 headers
- `##` - H2 headers
- `**` - Bold text
- Can be extended with more patterns if needed

## Testing

### Test Case 1: Markdown Response
```
Query: "What is the most common hazard?"
Expected: Full markdown answer in main display
Result: ‚úÖ Works
```

### Test Case 2: Plain Text Response
```
Query: "Count incidents"
Expected: Plain text in main display (no markdown)
Result: ‚úÖ Works (bold ** triggers switch)
```

### Test Case 3: Multiple Messages
```
Send message 1 ‚Üí Answer displays
Send message 2 ‚Üí New answer displays (no contamination)
Result: ‚úÖ Works (ref resets)
```

## Files Modified

- `src/pages/Agent2.tsx`
  - Added `isAnswerModeRef` for mode tracking
  - Updated `thinking_token` handler with markdown detection
  - Modified thinking dropdown to show "Thinking Process" label
  - Set thinking dropdown to collapsed by default
  - Added console log for mode switch debugging

## Future Improvements

### If Backend Can Be Modified
Ideal solution: Backend should send:
```typescript
// For internal reasoning
{ type: 'thinking_token', token: 'Let me analyze...' }

// For final answer
{ type: 'answer_token', token: '### Most Common...' }
```

### If Backend Cannot Be Modified
Current solution works perfectly! The frontend intelligently routes tokens based on content.

## Summary

‚úÖ **Final answers now display in main area** with proper markdown  
‚úÖ **Thinking process** (if any) shows in collapsed dropdown  
‚úÖ **Automatic detection** - no backend changes needed  
‚úÖ **Robust** - handles all token types and patterns  
‚úÖ **Clean UI** - proper separation of reasoning vs answer  

The system now correctly handles the backend's `thinking_token` stream by detecting when it switches from reasoning to formatted answer content, routing tokens to the appropriate display area.
